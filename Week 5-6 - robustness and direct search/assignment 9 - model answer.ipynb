{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-objective robust decision making (MORDM)\n",
    "\n",
    "\n",
    "This exercise demostrates the application of MORDM on the lake model, which was used in earlier exercises.\n",
    "\n",
    "MORDM has four main steps:\n",
    "\n",
    "(i)\t    **problem formulation** based on a systems analytical problem definition framework \n",
    "\n",
    "(ii)\t**searching** for candidate solutions that optimize multiple objectives by using multi-objective evolutionary algorithms \n",
    "\n",
    "(iii)\tgenerating an ensemble of scenarios to **explore** the effects of uncertainties \n",
    "\n",
    "(iv)\tusing **scenario discovery** to detect the vulnerabilities of candidate solutions and improving thecandidate solutions\n",
    "\n",
    "\n",
    "\n",
    "## Step 1: Problem formulation\n",
    "### Lake Model\n",
    "\n",
    "Remember the lake problem used in the assignments in previous weeks. The lake problem is a hypothetical case where the inhabitants of a lake town decide on the amount of annual pollution they release into a lake. It the pollution in the lake passes a threshold, it will suffer irreversible eutrophication.\n",
    "\n",
    "The lake problem has 4 **outcome indicators**: \n",
    "   - **max_P**: maximum pollution over time, to be minimized\n",
    "   - **utility**: economic benefits obtained from polluting the lake, to be maximized\n",
    "   - **inertia**: the percentage of significant annual changes in the anthropogenic pollution rate, to be maximized\n",
    "   - **reliability**: the percentage of years where the pollution level is below the critical threshold, to be maximized\n",
    "    \n",
    "See the lake model exercise for the formulation of these outcome variables.\n",
    "\n",
    "The lake problem is characterized by both stochastic uncertainty and **deep uncertainty**. The stochastic uncertainty arises from the natural inflow. To reduce this stochastic uncertainty, multiple replications are performed and the average over the replication is taken. Deep uncertainty is presented by uncertainty about the mean $\\mu$ and standard deviation $sigma$ of the lognormal distribution characterizing the natural inflow, the natural removal rate of the lake $\\beta$, the natural recycling rate of the lake $q$, and the discount rate $\\delta$. The table below specifies the ranges for the deeply uncertain factors, as well as their best estimate or default values. \n",
    "\n",
    "|Parameter\t|Range\t        |Default value|\n",
    "|-----------|--------------:|------------:|\n",
    "|$\\mu$    \t|0.01 – 0.05\t|0.02         |\n",
    "|$\\sigma$\t|0.001 – 0.005 \t|0.0017       |\n",
    "|$b$      \t|0.1 – 0.45\t    |0.42         |\n",
    "|$q$\t    |2 – 4.5\t    |2            |\n",
    "|$\\delta$\t|0.93 – 0.99\t|0.98         |\n",
    "\n",
    "\n",
    "The lake problem in previous assignments had 100 decision **levers**, meaning that the decision makers independently decide on the amount of anthropogenic pollution at every time step (100). Then a 'policy' was a set of values for these 100 levers, which you composed by sampling from the range [0, 0.1].   \n",
    "\n",
    "In this exercise, we will use a more advanced way of deciding on the amout of anhtropogenic polution. We will use a **closed loop** version of the lake model, meaning that $a_t$ (anthropogenic pollution) is dependent on $X_t$ (the pollution level at time t). For instance, the rate of anthropogenic pollutions is lowered if the pollution level is approaching a critical threshold. Here, we use \"cubic radial basis functions\" following [Quinn et al. 2017](http://www.sciencedirect.com/science/article/pii/S1364815216302250) and formulate $a_t$ as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "    a_{t} =  min\\Bigg(max\\bigg(\\sum\\limits_{j=1}^{n} w_{j}\\left\\vert{\\frac{X_{t,i}-c_{j}}{r_{j}}}\\right\\vert^3, 0.01\\bigg), 0.1\\Bigg) \\\\\n",
    "    s.t. \\\\\n",
    "    -2 \\leq c_{j} \\leq 2 \\\\\n",
    "    0 \\leq r_{j} \\leq 2 \\\\ \n",
    "    0 \\leq w_{j} \\leq 1 \\\\\n",
    "    \\sum\\limits_{j=1}^{n} w_{j} = 1\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "The parameters that define this function also define the pollution strategy over time. Hence, the decision **levers** are the five parameters of this functions, namely $c_1$, $c_2$, $r_1$, $r_2$ and $w_1$. ($w_2$ = 1 - $w_1$).\n",
    "\n",
    "Note:: i is index for the realization, given m realizations; j is the index for the radial basis function, given 2 radial basis functions. \n",
    "\n",
    "**To formulate this problem, do the following:**\n",
    "\n",
    "**1) Import the lake model function from dps_lake_model.py**\n",
    "\n",
    "**2) Create an ema_workbench interface for this problem, with corresponding uncertainties, levers and outcomes as specified above**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ema_workbench import (Model, RealParameter, ScalarOutcome)\n",
    "\n",
    "from dps_lake_model import lake_model\n",
    "\n",
    "model = Model('lakeproblem', function=lake_model)\n",
    "\n",
    "#specify uncertainties\n",
    "model.uncertainties = [RealParameter('b', 0.1, 0.45),\n",
    "                       RealParameter('q', 2.0, 4.5),\n",
    "                       RealParameter('mean', 0.01, 0.05),\n",
    "                       RealParameter('stdev', 0.001, 0.005),\n",
    "                       RealParameter('delta', 0.93, 0.99)]\n",
    "\n",
    "# set levers\n",
    "model.levers = [RealParameter(\"c1\", -2, 2),\n",
    "                RealParameter(\"c2\", -2, 2),\n",
    "                RealParameter(\"r1\", 0, 2),\n",
    "                RealParameter(\"r2\", 0, 2),\n",
    "                RealParameter(\"w1\", 0, 1)]\n",
    "\n",
    "#specify outcomes\n",
    "# note how we need to explicitely indicate the direction\n",
    "model.outcomes = [ScalarOutcome('max_P', kind=ScalarOutcome.MINIMIZE),\n",
    "                  ScalarOutcome('utility', kind=ScalarOutcome.MAXIMIZE),\n",
    "                  ScalarOutcome('inertia', kind=ScalarOutcome.MAXIMIZE),\n",
    "                  ScalarOutcome('reliability', kind=ScalarOutcome.MAXIMIZE)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Searching for candidate solutions\n",
    "\n",
    "In the second step of MORDM, candidate strategies are identified which are pareto optimal conditional on a reference scenario. These candiate strategies are identified through search with multi-objective evolutionary algorithms, that iteratively evaluate a large number of alternatives on multiple objectives until they find the best candidates. For instance, when we optimize the lake model levers, the lake model function will be called for each candidate evaluation, and the corresponding four objective values will be generated. \n",
    "\n",
    "Take the model interface developed in the previous step and use the optimization functionality of the workbench to identify the pareto approximate set of solutions. Try the following:\n",
    "* change the epsilon values between 0.01 and 0.1, what changes, why?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ema_workbench import MultiprocessingEvaluator, ema_logging\n",
    "from ema_workbench import (\n",
    "    Model,\n",
    "    Policy,\n",
    "    ema_logging,\n",
    "    MultiprocessingEvaluator,\n",
    "    SequentialEvaluator,\n",
    "    ScalarOutcome,\n",
    "    IntegerParameter,\n",
    "    optimize,\n",
    "    Scenario,\n",
    ")\n",
    "\n",
    "ema_logging.log_to_stderr(ema_logging.INFO)\n",
    "\n",
    "with MultiprocessingEvaluator(model) as evaluator:\n",
    "    results1 = evaluator.optimize(nfe=5e3, searchover='levers',\n",
    "                                 epsilons=[0.1,]*len(model.outcomes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with MultiprocessingEvaluator(model) as evaluator:\n",
    "    results2 = evaluator.optimize(nfe=5e3, searchover='levers',\n",
    "                                 epsilons=[0.01,]*len(model.outcomes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ema_workbench.analysis import parcoords\n",
    "\n",
    "data = results1.loc[:, [o.name for o in model.outcomes]]\n",
    "limits = parcoords.get_limits(data)\n",
    "limits.loc[0, ['utility', 'inertia', 'reliability', 'max_P']] = 0\n",
    "\n",
    "paraxes = parcoords.ParallelAxes(limits)\n",
    "paraxes.plot(data)\n",
    "paraxes.invert_axis('max_P')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = results2.loc[:, [o.name for o in model.outcomes]]\n",
    "limits = parcoords.get_limits(data)\n",
    "limits.loc[0, ['utility', 'inertia', 'reliability', 'max_P']] = 0\n",
    "\n",
    "paraxes = parcoords.ParallelAxes(limits)\n",
    "paraxes.plot(data)\n",
    "paraxes.invert_axis('max_P')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above parallel coordinate plots are consistent with our expectation. If we use a lower value for epsilon, we have many more solutions in the pareto approximate set.\n",
    "\n",
    "* change the number of function evaluations from 1000 to 10.000 (this requires using multiprocessing unless you are very patient). What is the difference? You can use  convergence as explained in assignment 7 for this\n",
    "\n",
    "Calculating hypervolume can be tricky. It requires specifying the minimum and maximum values that can occur. That is, we basically have to describe the hyperbox within which hypervolume needs to be calculated. There are various ways we can ensure this. For example we take the values quite broad to be certain. We can also add constraints to the outcome space. A common practice, however, is to use the minima and maxima from a reference set. This reference set can come from various sources. In some cases we have analytical solutions which we can use for the reference set. However, in many real world problems such analytical solutions don't exist. So, instead we use as a reference set, the set of best known solutions (*i.e.*, the final archive, or the set of archives merged across multiple seeds). This is also what I do below.\n",
    "\n",
    "Calculating hypervolume is computationally expensive. Therefore, it might sometimes be better to simply log the state of the achive at each generation, and only calculate hypervolume afterwards. See the [ArchiveLogger](http://emaworkbench.readthedocs.io/en/latest/ema_documentation/em_framework/optimization.html#ema_workbench.em_framework.optimization.ArchiveLogger) which you can use just like other convergence metrics for details\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ema_workbench.em_framework.optimization import (ArchiveLogger,\n",
    "                                                     EpsilonProgress)\n",
    "\n",
    "convergence_metrics = [\n",
    "    ArchiveLogger(\n",
    "        \"./archives\",\n",
    "        [l.name for l in model.levers],\n",
    "        [o.name for o in model.outcomes],\n",
    "        base_filename=\"assignment_9_1.tar.gz\",\n",
    "    ),\n",
    "    EpsilonProgress(),\n",
    "]\n",
    "\n",
    "with MultiprocessingEvaluator(model) as evaluator:\n",
    "    results, convergence = evaluator.optimize(nfe=1e3, searchover='levers',\n",
    "                                 convergence=convergence_metrics,\n",
    "                                 epsilons=[0.1,]*len(model.outcomes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archives = ArchiveLogger.load_archives(\"./archives/assignment_9_1.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ema_workbench import HypervolumeMetric\n",
    "from ema_workbench.em_framework.optimization import to_problem\n",
    "\n",
    "reference_set = results\n",
    "problem = to_problem(model, searchover=\"levers\")\n",
    "\n",
    "hv = HypervolumeMetric(reference_set, problem)\n",
    "\n",
    "hypervolume = [(nfe, hv.calculate(archive)) for nfe, archive in archives.items()]\n",
    "hypervolume.sort(key=lambda x:x[0])\n",
    "hypervolume = np.asarray(hypervolume)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2, sharex=True, figsize=(8,4))\n",
    "ax1.plot(convergence.nfe, convergence.epsilon_progress)\n",
    "ax1.set_ylabel('$\\epsilon$-progress')\n",
    "ax2.plot(hypervolume[:, 0], hypervolume[:, 1])\n",
    "ax2.set_ylabel('hypervolume')\n",
    "\n",
    "ax1.set_xlabel('number of function evaluations')\n",
    "ax2.set_xlabel('number of function evaluations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convergence_metrics = [\n",
    "    ArchiveLogger(\n",
    "        \"./archives\",\n",
    "        [l.name for l in model.levers],\n",
    "        [o.name for o in model.outcomes],\n",
    "        base_filename=\"assignment_9_2.tar.gz\",\n",
    "    ),\n",
    "    EpsilonProgress(),\n",
    "]\n",
    "\n",
    "with MultiprocessingEvaluator(model) as evaluator:\n",
    "    results, convergence = evaluator.optimize(nfe=1e4, searchover='levers',\n",
    "                                 convergence=convergence_metrics,\n",
    "                                 epsilons=[0.1,]*len(model.outcomes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archives = ArchiveLogger.load_archives(\"./archives/assignment_9_2.tar.gz\")\n",
    "\n",
    "reference_set = results\n",
    "problem = to_problem(model, searchover=\"levers\")\n",
    "\n",
    "hv = HypervolumeMetric(reference_set, problem)\n",
    "\n",
    "hypervolume = [(nfe, hv.calculate(archive)) for nfe, archive in archives.items()]\n",
    "hypervolume.sort(key=lambda x:x[0])\n",
    "hypervolume = np.asarray(hypervolume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, sharex=True, figsize=(8,4))\n",
    "ax1.plot(convergence.nfe, convergence.epsilon_progress)\n",
    "ax1.set_ylabel('$\\epsilon$-progress')\n",
    "ax2.plot(hypervolume[:, 0], hypervolume[:, 1])\n",
    "ax2.set_ylabel('hypervolume')\n",
    "\n",
    "ax1.set_xlabel('number of function evaluations')\n",
    "ax2.set_xlabel('number of function evaluations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the optimization has converged better in the second case with the higher number of function evaluations. Note also how in this particular case, search seems to stagnate before picking up again close to 10,000. So, to be certain, it would be good practice to use an even larger number of nfe (I would start wtih at least 50k, preferably even 250k), as shown below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convergence_metrics = [\n",
    "    ArchiveLogger(\n",
    "        \"./archives\",\n",
    "        [l.name for l in model.levers],\n",
    "        [o.name for o in model.outcomes],\n",
    "        base_filename=\"assignment_9_3.tar.gz\",\n",
    "    ),\n",
    "    EpsilonProgress(),\n",
    "]\n",
    "\n",
    "with MultiprocessingEvaluator(model) as evaluator:\n",
    "    results, convergence = evaluator.optimize(nfe=1e5, searchover='levers',\n",
    "                                 convergence=convergence_metrics,\n",
    "                                 epsilons=[0.1,]*len(model.outcomes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archives = ArchiveLogger.load_archives(\"./archives/assignment_9_3.tar.gz\")\n",
    "\n",
    "reference_set = results\n",
    "problem = to_problem(model, searchover=\"levers\")\n",
    "\n",
    "hv = HypervolumeMetric(reference_set, problem)\n",
    "\n",
    "hypervolume = [(nfe, hv.calculate(archive)) for nfe, archive in archives.items()]\n",
    "hypervolume.sort(key=lambda x:x[0])\n",
    "hypervolume = np.asarray(hypervolume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, sharex=True, figsize=(8,4))\n",
    "ax1.plot(convergence.nfe, convergence.epsilon_progress)\n",
    "ax1.set_ylabel('$\\epsilon$-progress')\n",
    "ax2.plot(hypervolume[:, 0], hypervolume[:, 1])\n",
    "ax2.set_ylim(ymin=0)\n",
    "ax2.set_ylabel('hypervolume')\n",
    "\n",
    "ax1.set_xlabel('number of function evaluations')\n",
    "ax2.set_xlabel('number of function evaluations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can more clearly see here, the early stalled search we saw was an artifact and we need at least 100k before we can have some confidence in convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**plot the tradeoffs you have found using a parallel axis plot**\n",
    "\n",
    "We can visualize these tradeoffs on a **parallel axis plots**. In these plots, each dimension is shown as a vertical axis. Each solution is represented by a line on this plot, which crosses the objective axes at the corresponsing value. You can use the [parcoords functionality](https://emaworkbench.readthedocs.io/en/latest/ema_documentation/analysis/parcoords.html) for this that comes with the ema_workbench. Ensure that the direction of desirability is the same for the four objectives.|\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = results.loc[:, [o.name for o in model.outcomes]]\n",
    "limits = parcoords.get_limits(data)\n",
    "limits.loc[0, ['utility', 'inertia', 'reliability', 'max_P']] = 0\n",
    "\n",
    "paraxes = parcoords.ParallelAxes(limits)\n",
    "paraxes.plot(data)\n",
    "paraxes.invert_axis('max_P')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What does this plot tell us about the tradeoffs and conflicting objectives?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max_P is negatively correlated with utility and intertia, and positevely with reliability. if we look at inertia, there are two solutions which sacrifice intertia for higher scores on reliability. These might reflect a different correlation structure, or be evidence of incomplete convergence. \n",
    "\n",
    "\n",
    "## Step 3: Re-evaluate candidate solutions under uncertainty\n",
    "\n",
    "We now have a large number of candidate solutions (policies), we can re-evaluate them over the various deeply uncertain factors to assess their robustness against uncertainties.\n",
    "\n",
    "For this robustness evaluation, we need to explore the scenarios for each solution. It means that, if we would like to run for instance 1000 scenarios for each solution, we might have to execute a very large number of runs.\n",
    "\n",
    "Here, to simplify the case, let's suppose that decision makers have a hard constrain on *reliability*. No solution with less than 90% reliability is acceptable for them. Therefore, we can reduce the size of the solution set according to this constraint. \n",
    "\n",
    "**Apply this constraint of reliability on the results, and create a new dataframe named new_results**\n",
    "\n",
    "**From new_results, which is the reduced dataframe of candidate solutions, make a list of policies in a format that can be inputed to the *perform_experiments* function of the EMA workbench.**\n",
    "\n",
    "*hint: you need to transform each policy to a dict, and then use this dict as input for the Policy class that comes with the workbench*\n",
    "\n",
    "There are various ways to do it. One way is to use logical indexing. Basically, create a boolean vector that indicates for each row if the constraint is met or not. Next, we can use this as an index on the dataframe to get only the rows for which the index is true.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logical = results.reliability > 0.9\n",
    "np.sum(logical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only have one solution that meets the constraints. Given that below we need to calculate regret (which requires more than one solution), let's try a slighly less stringent constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logical = results.reliability > 0.75\n",
    "np.sum(logical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 solutions is better, so we can proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[logical]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policies = results[logical]\n",
    "policies = policies.drop([o.name for o in model.outcomes], axis=1)\n",
    "policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ema_workbench import Policy\n",
    "\n",
    "policies_to_evaluate = []\n",
    "\n",
    "for i, policy in policies.iterrows():\n",
    "    policies_to_evaluate.append(Policy(str(i), **policy.to_dict()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perform 1000 scenarios for each of the policy options. Depending on how many solutions are left after implementing the constraint, consider using multiprocessing or ipyparallel to speed up calculations.**\n",
    "\n",
    "If you want to use ipyparallel, don't forget to start ipcluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_scenarios = 1000\n",
    "with MultiprocessingEvaluator(model) as evaluator:\n",
    "    results = evaluator.perform_experiments(n_scenarios,\n",
    "                                            policies_to_evaluate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now evaluate the **robustness** of each of the policy options based on these scenario results. We can calculate the robustness of a policy option in terms of its performance on an outcome indicator across the 1000 scenarios. In other words, we can identify how robust a policy is in terms of each outcome indicator, and investigate the robustness tradeoffs.  \n",
    "\n",
    "There are multiple metrics to quantify robustness. On of them is the *signal to noise ratio*, which is simply the mean of a dataset divided by its standard deviation. For instance, for an outcome indicator to be maximized, we prefer a high average value across the scenarios, and a low standard deviation, implying a narrow range of uncertainty about the outcomes. Therefore, we want to maximize the signal-to-noise ratio. For an outcome indicator to be minimized, a lower mean and a lower standard deviation is preferred. Therefore the formulation should be different.\n",
    "\n",
    "**Write a function to calculate the signal-to-noise ratio for both kinds of outcome indicators. Calculate the signal-to-noise ratios for each outcome and each policy option. Plot the tradeoffs on a parallel plot. Which solutions look like a good compromise policy?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s_to_n(data, direction):\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    \n",
    "    if direction==ScalarOutcome.MAXIMIZE:\n",
    "        return mean/std\n",
    "    else:\n",
    "        return mean*std\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various ways in which we could calculate the signal to noise ratio. Here, I choose to iterate over the policy first. Next, I iterate over the outcomes. For each outcome, I only retrieve the results associated with the current policy. Next, I can calculate the signal to noise ratio. \n",
    "\n",
    "I am reusing the direction that we already specified for each outcome of interest, to avoid duplicating code. Note that I enabled this already with how I defined the s_to_n function above.\n",
    "\n",
    "To make visualization easy, I transform all my results into a dataframe at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments, outcomes = results\n",
    "\n",
    "overall_scores = {}\n",
    "for policy in np.unique(experiments['policy']):\n",
    "    scores = {}\n",
    "    \n",
    "    logical = experiments['policy']==policy\n",
    "    \n",
    "    for outcome in model.outcomes:\n",
    "        value  = outcomes[outcome.name][logical]\n",
    "        sn_ratio = s_to_n(value, outcome.kind)\n",
    "        scores[outcome.name] = sn_ratio\n",
    "    overall_scores[policy] = scores\n",
    "scores = pd.DataFrame.from_dict(overall_scores).T\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ema_workbench.analysis import parcoords\n",
    "\n",
    "data = scores\n",
    "limits = parcoords.get_limits(data)\n",
    "limits.loc[0, ['utility', 'inertia', 'reliability', 'max_P']] = 0\n",
    "\n",
    "paraxes = parcoords.ParallelAxes(limits)\n",
    "paraxes.plot(data)\n",
    "paraxes.invert_axis('max_P')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The ideal solution would be a a high Signal to Noise score for reliability, inertia, and utility, and a low score for max_P. Such a solution clearly does not exist. The best solution for max_P and reliability score low on inertia and utility. No compromise solution can thus be found in this case. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another robustness metric is **maximum regret**, calculated again for each policy and for each outcome indicator. *Regret* is defined for each policy under each scenario, as the difference between the performance of the policy in a specific scenario and the berformance of a no-regret (i.e. best possible result in that scenario) or reference policy. The *maximum regret*  is then the maximum of such regret values across all scenarios. We of course favor policy options with low *maximum regret* values. \n",
    "\n",
    "**Write a function to calculate the maximum regret for both kinds of outcome indicators. Calculate the maximum regret values for each outcome and each policy option. Plot the tradeoffs on a parallel plot. Which solutions look like a good compromise policy?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_regret(data, best):\n",
    "    return np.abs(best-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regret is the performance difference between the best possible outcome in a scenario across policies, the the observed outcome for a given policy. We have in this case both minimization and maximization. Best means the lowest in case of mimimization, and heighest in case of maximization. To avoid having to explicitly account for this in how we calculate the difference, we can simply take the absolute value of the difference. In this case, max_P will return negative regret values for `best-data`, so by taking the absolute value, we fix this\n",
    "\n",
    "The next part of the code is probably the most tricky part. We need to find the best possible outcome for each scenario. We could do this by iterating over the scenario_id column in the experiment array. But we can also use pandas instead as done below. Wat we do is the following:\n",
    "1. we create a dataframe with the outcome, the name of the policy and the scenario. This is a so called long-form representation of the data\n",
    "2. We want to have the results for each policy side by side so we can take the max, or min accross the column. The pivot method on the DataFrame does this for us\n",
    "3. We take the maximum or minimum accross the row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments, outcomes = results\n",
    "\n",
    "overall_regret = {}\n",
    "max_regret = {}\n",
    "for outcome in model.outcomes:\n",
    "    policy_column = experiments['policy']\n",
    "    \n",
    "    # create a DataFrame with all the relevent information\n",
    "    # i.e., policy, scenario_id, and scores\n",
    "    data = pd.DataFrame({outcome.name: outcomes[outcome.name], \n",
    "                         \"policy\":experiments['policy'],\n",
    "                         \"scenario\":experiments['scenario']})\n",
    "    \n",
    "    # reorient the data by indexing with policy and scenario id\n",
    "    data = data.pivot(index='scenario', columns='policy')\n",
    "    \n",
    "    # flatten the resulting hierarchical index resulting from \n",
    "    # pivoting, (might be a nicer solution possible)\n",
    "    data.columns = data.columns.get_level_values(1)\n",
    "    \n",
    "    # we need to control the broadcasting. \n",
    "    # max returns a 1d vector across scenario id. By passing\n",
    "    # np.newaxis we ensure that the shape is the same as the data\n",
    "    # next we take the absolute value\n",
    "    #\n",
    "    # basically we take the difference of the maximum across \n",
    "    # the row and the actual values in the row\n",
    "    #\n",
    "    outcome_regret = (data.max(axis=1).values[:, np.newaxis] - data).abs()\n",
    "    \n",
    "    overall_regret[outcome.name] = outcome_regret\n",
    "    max_regret[outcome.name] = outcome_regret.max()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_regret = pd.DataFrame(max_regret)\n",
    "sns.heatmap(max_regret/max_regret.max(), cmap='viridis', annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "colors = sns.color_palette()\n",
    "\n",
    "data = max_regret\n",
    "\n",
    "# makes it easier to identify the policy associated with each line\n",
    "# in the parcoords plot\n",
    "# data['policy'] = data.index.astype(\"float64\")\n",
    "\n",
    "limits = parcoords.get_limits(data)\n",
    "limits.loc[0, ['utility', 'inertia', 'reliability', 'max_P']] = 0\n",
    "\n",
    "paraxes = parcoords.ParallelAxes(limits)\n",
    "for i, (index, row) in enumerate(data.iterrows()):\n",
    "    paraxes.plot(row.to_frame().T, label=str(index), color=colors[i])\n",
    "paraxes.legend()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we clearly see a tradeoff between robustsness on maximum pollution and utility versus inertia and reliability. There is no compromise solution.\n",
    "\n",
    "Note that we have been looking at the maximum regret. I also saved the distribution of regret over the set of scenarios. So let's visualize this and see what we can learn from it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "policy_regret = defaultdict(dict)\n",
    "for key, value in overall_regret.items():\n",
    "    for policy in value:\n",
    "        policy_regret[policy][key] = value[policy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this generates a 2 plots with a shared y and x axis\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10,5), \n",
    "                         sharey=True, sharex=True)\n",
    "\n",
    "# to ensure easy iteration over the axes grid, we turn it\n",
    "# into a list. Because there are four plots, I hard coded\n",
    "# this. \n",
    "\n",
    "\n",
    "# zip allows us to zip together the list of axes and the list of \n",
    "# key value pairs return by items. If we iterate over this\n",
    "# it returns a tuple of length 2. The first item is the ax\n",
    "# the second items is the key value pair.\n",
    "for ax, (policy, regret) in zip(axes, policy_regret.items()):\n",
    "    data = pd.DataFrame(regret)\n",
    "\n",
    "    # we need to scale the regret to ensure fair visual\n",
    "    # comparison. We can do that by divding by the maximum regret\n",
    "    data = data/max_regret.max(axis=0)\n",
    "    sns.boxplot(data=data, ax=ax)\n",
    "    \n",
    "    # removes top and left hand black outline of axes\n",
    "    sns.despine()\n",
    "    \n",
    "    # ensure we know which policy the figure is for\n",
    "    ax.set_title(str(policy))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is in line with the maximum regret parallel coordinates plot, but we get some more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have an understanding of which solutions have decent robustness using 2 different robustness metrics. \n",
    "\n",
    "A related but different question is to assess the uncertain conditions under which we get poor performance. For this, we can use scenario discovery. Since we want to identify the uncertainties only, we can remove the policy and lever columns from the experiments DataFrame. \n",
    "\n",
    "**Perform Scenario Discovery, focussed on understanding the conditions under which utility is lower than 0.35**\n",
    "\n",
    "note that it might happen that no scenario exists where this is the case. If so, increase the threshold to say 0.5 instead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ema_workbench.analysis import prim\n",
    "\n",
    "x = experiments.drop(columns=['policy', 'c1','c2', 'r1', 'r2', 'w1'])\n",
    "y = outcomes['utility'] < 0.5\n",
    "\n",
    "prim_alg = prim.Prim(x, y, threshold=0.5)\n",
    "box = prim_alg.find_box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box.inspect_tradeoff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the choice for box 42 is somewhat arbitrary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box.inspect(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here that low utility is driven by delta. In practice, the next step is to ask why this happens. What is it about this delta parameter that explains the low performance on utility. Once this is clear, the next step is to ask what can be done about the situation to resolve this poor performance in case of a low delta parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing remarks\n",
    "\n",
    "This notebook has taken you through one iteration of the MORDM cycle. First, you used an MOEA to find a set of candidate solutions. Next, you re-evaluated a selected subset of these under deep uncertainty and calculated several robustness metrics. You also used scenario discovery to understand the conditions under which poor performance (here just for one objective) occurs. In practice, this would be the starting point for a next iteration. Once you understand why poor performance happens, you have to go back to how the problem is being framed and rethinkin this framing. For example, here the problem is framed in terms of polluting a lake. But why could we not expand the solution space by adding options for either water treatment of some nature based interventions in the lake aimed at improving its carying capacity for phosphorus? If these turn out to be serious options, this would start a next iteration with a reframed problem with additional levers. Again optimization can be used to find promising solutions for this reframed problem. This iterative process continues untill a satisfactory set of solutions is being found that can facilitiate a negotiation between the various stakeholders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
